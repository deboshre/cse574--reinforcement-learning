{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bonus1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "Hcka8zZTIB86",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Solving cartpole game with DQN**"
      ]
    },
    {
      "metadata": {
        "id": "6GC9-DCXFip4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The goal of the cartpole game is to balance the pole connected with one joint on top of a moving cart. There are 4 kinds of information given by the state, such as angle of the pole and position of the cart. An agent can move the cart by performing a series of actions of 0 or 1 to the cart, pushing it left or right."
      ]
    },
    {
      "metadata": {
        "id": "Npo3u_IuAiTN",
        "colab_type": "code",
        "outputId": "0107284d-0862-40a5-fb06-42a2f29870ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "cell_type": "code",
      "source": [
        " !pip -q install gym #installing all the required packages\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "%matplotlib inline\n",
        "\n",
        "EPISODES = 500\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95    # discount rate  to calculate the future discounted reward.\n",
        "        self.epsilon = 1.0  # exploration rate :this is the rate in which an agent randomly decides its action rather than prediction.\n",
        "        self.epsilon_min = 0.01 #we want the agent to explore at least this amount.\n",
        "        self.epsilon_decay = 0.995 #we want to decrease the number of explorations as it gets good at playing games.\n",
        "        self.learning_rate = 0.001 #Determines how much neural net learns in each iteration.\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        # Neural Net for Deep-Q learning Model\n",
        "        model = Sequential()\n",
        "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(24, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        model.compile(loss='mse',\n",
        "                      optimizer=Adam(lr=self.learning_rate))\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done): #remember function will store states, actions and resulting rewards to the memory\n",
        "        self.memory.append((state, action, reward, next_state, done)) #done is just a boolean that indicates if the state is the final state.\n",
        "\n",
        "    def act(self, state): #act function works on the principle that initially the agent goes rogue but after training on mutiple iterations \n",
        "    #When it is not deciding the action randomly, the agent will predict the reward value based on the current state and pick the action that will give the highest reward. \n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])  # returns action\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size) #The above code will make minibatch, which is just a randomly sampled elements of the memories of size batch_size. \n",
        "        for state, action, reward, next_state, done in minibatch: #We set the batch size as 32.\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0])) \n",
        "            target_f = self.model.predict(state) #predict() function on the model will predict the reward of current state based on the data we trained.\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0) #fit() method feeds input and output pairs to the model.\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make('CartPole-v1')\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.n\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "    epsilons =[]\n",
        "    done = False\n",
        "    batch_size = 32\n",
        "\n",
        "    for e in range(EPISODES): #episode: a number of games we want the agent to play.\n",
        "        state = env.reset() ## reset state in the beginning of each game\n",
        "        state = np.reshape(state, [1, state_size])\n",
        "        for time in range(500): ## time_t represents each frame of the game\n",
        "        # Our goal is to keep the pole upright as long as possible until score of 500\n",
        "        # the more time_t the more score\n",
        "            #env.render()\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            reward = reward if not done else -10\n",
        "            next_state = np.reshape(next_state, [1, state_size])\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state ## makes next_state the new current state for the next frame.\n",
        "            if done: #done becomes True when the game ends\n",
        "                print(\"episode: {}/{}, score: {}, e: {:.2}\"\n",
        "                      .format(e, EPISODES, time, agent.epsilon))\n",
        "                break\n",
        "            if len(agent.memory) > batch_size:\n",
        "                agent.replay(batch_size)\n",
        "        epsilons.append(agent.epsilon)\n",
        "        if e % 10 == 0:\n",
        "            agent.save(\"cartpole-dqn.h5\")\n",
        "    agent.load(\"cartpole-dqn.h5\")\n",
        "   \n",
        "\n",
        "\n",
        "    plt.figure(figsize=(8, 6), dpi=80)\n",
        "    plt.title(\"Epsilon\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Epsilon value\")\n",
        "    plt.plot(epsilons)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode: 0/500, score: 15, e: 1.0\n",
            "episode: 1/500, score: 14, e: 1.0\n",
            "episode: 2/500, score: 25, e: 0.89\n",
            "episode: 3/500, score: 20, e: 0.8\n",
            "episode: 4/500, score: 28, e: 0.7\n",
            "episode: 5/500, score: 14, e: 0.65\n",
            "episode: 6/500, score: 13, e: 0.61\n",
            "episode: 7/500, score: 10, e: 0.58\n",
            "episode: 8/500, score: 8, e: 0.56\n",
            "episode: 9/500, score: 11, e: 0.53\n",
            "episode: 10/500, score: 10, e: 0.5\n",
            "episode: 11/500, score: 12, e: 0.47\n",
            "episode: 12/500, score: 23, e: 0.42\n",
            "episode: 13/500, score: 12, e: 0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lDaJ6HayZNOj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}